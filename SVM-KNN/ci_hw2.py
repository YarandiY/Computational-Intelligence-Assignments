# -*- coding: utf-8 -*-
"""CI_HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14HwUGASkpoX-nADkMKNIEr4CdC7LZWJA

<h1>
Computational Intelligence Assignment 
</h1>
<p>
Implement SVM and KNN without any library. 
</p>
"""

# upload data
from google.colab import files
uploaded = files.upload()

"""First of all, I've used `google.colab` to uploud my data and read them by `pandas`"""

import io
import pandas as pd
import numpy as np
import sklearn.model_selection
features = ["Age", "Workclass", "fnlwgt", "Education", "Education-Num", "Martial Status",
              "Occupation", "Relationship", "Race", "Sex", "Capital Gain", "Capital Loss",
              "Hours per week", "Country", "Target"] 
df = pd.read_csv(io.StringIO(uploaded['adult.csv'].decode('utf-8')), names=features, sep=r'\s*,\s*', engine='python')
df.head()

"""
<h3>
The data must be preprocessed before using them for training 
</h3>


---


<p>
<ul>
<li>Missing data</li>
<p>
If missing data of every column is more than 10% so I'll remove the column otherwise I'll remove the records</p>"""

def preprocess_missing_data(data):
  for c in data.columns:
    number_of_missing_data = data[c].isin(["?"]).sum()
    if number_of_missing_data > 0:
      percentage = float(number_of_missing_data)/data.shape[0]*100
      print("percentage of records with missig data: ", percentage,"%")
      if percentage < 10:
        data = data[data[c]!="?"]
      else :
        data = data.drop(c, axis=1)
  return data


df = preprocess_missing_data(df)
df.head()

"""<ul>
<li>
Normalize Numerical Features
</li>
"""

def normalize_numerical_features(data):
  data_nums = data.select_dtypes(include=np.number)
  for c in data_nums.columns:
        max = data[c].max()
        min = data[c].min()
        data[c] = (data[c]-min)/(max-min)
  return data

df = normalize_numerical_features(df)
df.head()

"""<ul>
<li>One-Hot Encoding</li>
<p>for preprocess non-numerical feature except the Target!</p>
</ul>
"""

def one_hot_encoding(data):
  # Y:
  data['Target'] = data['Target'].replace('<=50K', 0).replace('>50K', 1)
  # X:
  data_cat = data.select_dtypes(exclude=np.number)
  data = pd.get_dummies(data, columns=data_cat.columns)
  return data

df = one_hot_encoding(df)
df.tail()

"""
<p>Divide train, validation and test set from x and y</p>"""

X = df.drop("Target", axis=1)
Y = np.array(df["Target"])
x_train,x_test,y_train,y_test=sklearn.model_selection.train_test_split(X, Y, test_size=0.2)
x_train.head()
x_train,x_validation,y_train,y_validation=sklearn.model_selection.train_test_split(x_train, y_train, test_size=0.25)

"""**SVM**


---


"""

from sklearn.svm import SVC

def SVM_prediction(x_train, y_train, x_test):
  svclassifier = SVC(kernel='linear')
  svclassifier.fit(x_train, y_train)
  y_pred = svclassifier.predict(x_test)
  return y_pred


y_pred = SVM_prediction(x_train, y_train, x_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test,y_pred)
print("accuracy : ", accuracy, "%")

"""
**KNN**


---



"""

def find_destinations(x, x_train):
  destinations = []
  # for i in range(1, len(x_train)):
  #   dest = np.sqrt(np.sum(np.power(np.array(x) - np.array(x_train.iloc[i]), 2)))
  #   destinations.append(dest)
  dest = np.sqrt(np.sum(np.power(np.array(x) - np.array(x_train), 2), axis=1))
  return dest
  
destinations = find_destinations(x_test.iloc[1], x_train)
destinations

def predicate_y(y_train, indexes):
  sum = 0
  for i in indexes:
    sum += y_train[i]
  possibility = sum/len(indexes)
  return possibility

def KNN(x_train, y_train, x_validation, y_validation, K=5):
  y_pre = []
  for i in range(0, len(x_validation)):
    dests = find_destinations(x_validation.iloc[i], x_train)
    dests = np.array(dests)
    indexes = dests.argsort()[:K]
    pos = predicate_y(y_train, indexes)
    pre = 0 if pos < 0.5 else 1
    if i % 2000 == 0 :
      print("predicate : ", pre , "  actual : ", y_validation[i])
    y_pre.append(pre)
  return y_pre

"""
Find the best K for the algorithm


---

"""

max_accuracy = 0
best_K = 0;
for k in range(5, 30, 5):
  print("K=", k)
  y_pre = KNN(x_train, y_train, x_validation, y_validation, k)
  accuracy = np.sum(y_pre == y_validation) / len(y_pre) * 100
  print("Accuracy: %.2F%% " %accuracy)
  print("")
  if accuracy > max_accuracy:
    max_accuracy = accuracy
    best_K = k

y_pre = KNN(x_train, y_train, x_test, y_test, best_K) 
accuracy = np.sum(y_pre == y_test) / len(y_pre) * 100
print("Accuracy with K = ", best_K, " : %.2F%% " %accuracy)

